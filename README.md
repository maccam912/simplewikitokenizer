Created with https://github.com/smol-ai/developer

This will download, extract, and train a tokenizer based on the text in Simple English Wikipedia. The goal is a super small tokenizer, ~4000 tokens, to speed up training time on a few very common words.
